{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc713c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa4909c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한글',\n",
       " '형태소',\n",
       " '분석',\n",
       " '기',\n",
       " '(',\n",
       " '오',\n",
       " '케이티',\n",
       " ')',\n",
       " '로',\n",
       " '테스트',\n",
       " '해보았습니다',\n",
       " '정상',\n",
       " '설치',\n",
       " '및',\n",
       " '동작',\n",
       " '이',\n",
       " '잘',\n",
       " '됩니다',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs('한글 형태소 분석기(오케이티)로 테스트 해보았습니다 정상 설치 및 동작이 잘 됩니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ee4917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eunjeon import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52afd4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['명사', '해당', '형태소', '추출']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab = Mecab() \n",
    "mecab.pos(\"품사 태깅을 지원합니다\") \n",
    "mecab.morphs(\"형태소 분리를 지원합니다\") \n",
    "mecab.nouns(\"명사에 해당하는 형태소만 추출합니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f20b7",
   "metadata": {},
   "source": [
    "## TF-IDF & 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68881085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446e902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')\n",
    "sample_submission = pd.read_csv('dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5318d5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features: 100, Accuracy: 0.726\n",
      "max_features: 200, Accuracy: 0.757\n",
      "max_features: 300, Accuracy: 0.79\n",
      "max_features: 400, Accuracy: 0.803\n",
      "max_features: 500, Accuracy: 0.811\n",
      "max_features: 600, Accuracy: 0.814\n",
      "max_features: 700, Accuracy: 0.821\n",
      "max_features: 800, Accuracy: 0.833\n",
      "max_features: 900, Accuracy: 0.843\n",
      "max_features: 1000, Accuracy: 0.844\n",
      "max_features: 1100, Accuracy: 0.846\n",
      "max_features: 1200, Accuracy: 0.852\n",
      "max_features: 1300, Accuracy: 0.849\n",
      "max_features: 1400, Accuracy: 0.849\n",
      "max_features: 1500, Accuracy: 0.851\n",
      "max_features: 1600, Accuracy: 0.847\n",
      "max_features: 1700, Accuracy: 0.852\n",
      "max_features: 1800, Accuracy: 0.855\n",
      "max_features: 1900, Accuracy: 0.859\n",
      "max_features: 2000, Accuracy: 0.858\n",
      "max_features: 2100, Accuracy: 0.858\n",
      "max_features: 2200, Accuracy: 0.858\n",
      "max_features: 2300, Accuracy: 0.862\n",
      "max_features: 2400, Accuracy: 0.856\n",
      "max_features: 2500, Accuracy: 0.859\n",
      "max_features: 2600, Accuracy: 0.86\n",
      "max_features: 2700, Accuracy: 0.862\n",
      "max_features: 2800, Accuracy: 0.864\n",
      "max_features: 2900, Accuracy: 0.867\n",
      "max_features: 3000, Accuracy: 0.869\n",
      "max_features: 3100, Accuracy: 0.868\n",
      "max_features: 3200, Accuracy: 0.871\n",
      "max_features: 3300, Accuracy: 0.872\n",
      "max_features: 3400, Accuracy: 0.872\n",
      "max_features: 3500, Accuracy: 0.871\n",
      "max_features: 3600, Accuracy: 0.871\n",
      "max_features: 3700, Accuracy: 0.872\n",
      "max_features: 3800, Accuracy: 0.871\n",
      "max_features: 3900, Accuracy: 0.87\n",
      "max_features: 4000, Accuracy: 0.87\n",
      "max_features: 4100, Accuracy: 0.87\n",
      "max_features: 4200, Accuracy: 0.869\n",
      "max_features: 4300, Accuracy: 0.87\n",
      "max_features: 4400, Accuracy: 0.872\n",
      "max_features: 4500, Accuracy: 0.869\n",
      "max_features: 4600, Accuracy: 0.874\n",
      "max_features: 4700, Accuracy: 0.872\n",
      "max_features: 4800, Accuracy: 0.871\n",
      "max_features: 4900, Accuracy: 0.87\n",
      "max_features: 5000, Accuracy: 0.867\n",
      "max_features: 5100, Accuracy: 0.867\n",
      "max_features: 5200, Accuracy: 0.868\n",
      "max_features: 5300, Accuracy: 0.867\n",
      "max_features: 5400, Accuracy: 0.868\n",
      "max_features: 5500, Accuracy: 0.868\n",
      "max_features: 5600, Accuracy: 0.867\n",
      "max_features: 5700, Accuracy: 0.866\n",
      "max_features: 5800, Accuracy: 0.866\n",
      "max_features: 5900, Accuracy: 0.865\n",
      "max_features: 6000, Accuracy: 0.867\n",
      "max_features: 6100, Accuracy: 0.866\n",
      "max_features: 6200, Accuracy: 0.868\n",
      "max_features: 6300, Accuracy: 0.867\n",
      "max_features: 6400, Accuracy: 0.868\n",
      "max_features: 6500, Accuracy: 0.868\n",
      "max_features: 6600, Accuracy: 0.868\n",
      "max_features: 6700, Accuracy: 0.87\n",
      "max_features: 6800, Accuracy: 0.867\n",
      "max_features: 6900, Accuracy: 0.868\n",
      "max_features: 7000, Accuracy: 0.868\n",
      "max_features: 7100, Accuracy: 0.869\n",
      "max_features: 7200, Accuracy: 0.868\n",
      "max_features: 7300, Accuracy: 0.869\n",
      "max_features: 7400, Accuracy: 0.869\n",
      "max_features: 7500, Accuracy: 0.869\n",
      "max_features: 7600, Accuracy: 0.869\n",
      "max_features: 7700, Accuracy: 0.868\n",
      "max_features: 7800, Accuracy: 0.868\n",
      "max_features: 7900, Accuracy: 0.868\n",
      "max_features: 8000, Accuracy: 0.867\n",
      "max_features: 8100, Accuracy: 0.867\n",
      "max_features: 8200, Accuracy: 0.867\n",
      "max_features: 8300, Accuracy: 0.867\n",
      "max_features: 8400, Accuracy: 0.868\n",
      "max_features: 8500, Accuracy: 0.868\n",
      "max_features: 8600, Accuracy: 0.869\n",
      "max_features: 8700, Accuracy: 0.868\n",
      "max_features: 8800, Accuracy: 0.867\n",
      "max_features: 8900, Accuracy: 0.868\n",
      "max_features: 9000, Accuracy: 0.867\n",
      "max_features: 9100, Accuracy: 0.869\n",
      "max_features: 9200, Accuracy: 0.869\n",
      "max_features: 9300, Accuracy: 0.868\n",
      "max_features: 9400, Accuracy: 0.867\n",
      "max_features: 9500, Accuracy: 0.868\n",
      "max_features: 9600, Accuracy: 0.87\n",
      "max_features: 9700, Accuracy: 0.87\n",
      "max_features: 9800, Accuracy: 0.87\n",
      "max_features: 9900, Accuracy: 0.871\n"
     ]
    }
   ],
   "source": [
    "# tf-idf 벡터화\n",
    "for max_features in range(100, 10000, 100):\n",
    "    vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1, 3), max_features=max_features)\n",
    "    \n",
    "    X = train['document']\n",
    "    y = np.array(train.label)\n",
    "    \n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_eval = vectorizer.transform(X_eval)\n",
    "    lgs = LogisticRegression(class_weight='balanced')\n",
    "    lgs.fit(X_train, y_train)\n",
    "    predicted = lgs.predict(X_eval)\n",
    "    print(f\"max_features: {max_features}, Accuracy: {lgs.score(X_eval, y_eval)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e0358eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train)\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train\u001b[38;5;241m.\u001b[39mlabel)\n\u001b[1;32m----> 8\u001b[0m X_train_2 \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m X_test_2 \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test))\n\u001b[0;32m     10\u001b[0m lgs \u001b[38;5;241m=\u001b[39m LogisticRegression(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[38;5;124;03m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[0;32m   2059\u001b[0m \n\u001b[0;32m   2060\u001b[0m \u001b[38;5;124;03mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m-> 2077\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# 4600 일 때 가장 큰 정확도 0.874 를 얻었습니다.\n",
    "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3), max_features=9900)\n",
    "\n",
    "X = train['document']\n",
    "y = np.array(train.label)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X)\n",
    "X_test = vectorizer.transform(test['document'])\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y)\n",
    "predicted = lgs.predict(X_test)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train['document'])\n",
    "X_test = vectorizer.transform(test['document'])\n",
    "y = np.array(train.label)\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y)\n",
    "predicted = lgs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9248e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('dataset/sample_submission.csv')\n",
    "sample_submission.loc[:, 'label'] = predicted\n",
    "\n",
    "sample_submission.to_csv('sample_submission_day1_logistic2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b65f0",
   "metadata": {},
   "source": [
    "## 전처리 추가 - 정규표현식\n",
    "- 각종 특수문자 확인, 한글만 남기고 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0699f6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>괜찮은 음악영화가 또 나왔군요!!! 따뜻한 겨울이 될 것 같아요~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>아무래도 20년도지난작품이라 지금보기는너무유치하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>지금까지의 영화들이 그랬듯. 이 영화역시 일본에 대한 미화는 여전하다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                 document  label\n",
       "0   1                영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐      0\n",
       "1   2             히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯...      1\n",
       "2   3     괜찮은 음악영화가 또 나왔군요!!! 따뜻한 겨울이 될 것 같아요~      1\n",
       "3   4              아무래도 20년도지난작품이라 지금보기는너무유치하다      0\n",
       "4   5  지금까지의 영화들이 그랬듯. 이 영화역시 일본에 대한 미화는 여전하다.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a2fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['document'] = train['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d908fd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>괜찮은 음악영화가 또 나왔군요 따뜻한 겨울이 될 것 같아요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>아무래도 년도지난작품이라 지금보기는너무유치하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>지금까지의 영화들이 그랬듯 이 영화역시 일본에 대한 미화는 여전하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                               document  label\n",
       "0   1              영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐      0\n",
       "1   2              히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯      1\n",
       "2   3       괜찮은 음악영화가 또 나왔군요 따뜻한 겨울이 될 것 같아요      1\n",
       "3   4              아무래도 년도지난작품이라 지금보기는너무유치하다      0\n",
       "4   5  지금까지의 영화들이 그랬듯 이 영화역시 일본에 대한 미화는 여전하다      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554be988",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['document'] = test['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c16159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>시간 때우기 좋은 영화 지루함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>훈훈한 정이 느껴지는 영화 가족끼리 드라마 보듯이 보면 딱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>멋있는 영화입니다 잊을 수 없는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>너무 감동적이네요 펑펑 울었습니다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                          document\n",
       "0   1                  시간 때우기 좋은 영화 지루함\n",
       "1   2  훈훈한 정이 느껴지는 영화 가족끼리 드라마 보듯이 보면 딱\n",
       "2   3                                  \n",
       "3   4                 멋있는 영화입니다 잊을 수 없는\n",
       "4   5                너무 감동적이네요 펑펑 울었습니다"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286534e",
   "metadata": {},
   "source": [
    "## 전처리 추가 - 불용어 제거\n",
    "- 불용어란 의미가 없는 단어를 의미 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af8dfcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84e6c8",
   "metadata": {},
   "source": [
    "- 한국어 형태소 분석기 중 Okt 형태소 분석기를 사용하여 제거\n",
    "- Okt(Open Korean Text)는 트위터에서 만든 오픈소스 한국어 처리기인 twitter-korean-text를 이어받아 만들고 있는 프로젝트라고 합니다. 형태소 분석기 중에선 시간이 빠르진 않지만 합리적인 시간을 가지고 있어서 자주 사용된다고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3d0cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # 진행률 프로세스 바 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6284f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c96f490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:16<00:00, 305.08it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "okt = Okt()\n",
    "for sentence in tqdm(train['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) #토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]#불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence) # 불용어 제거된 것 append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfa6b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['영상', '이나', '음악', '이쁘다', '해도', '미화', '시키다', '불륜', '일', '뿐'], ['히치콕', '영화', '보다', '분명', '박수', '치다'], ['괜찮다', '음악', '영화', '또', '나오다', '따뜻하다', '겨울', '되다', '것', '같다']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ea973b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:10<00:00, 469.23it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for sentence in tqdm(test['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) #토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]#불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence) # 불용어 제거된 것 append"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9655188",
   "metadata": {},
   "source": [
    "## 전처리 -  정수 인코딩\n",
    "- 텍스트를 숫자로 바꾸는 기법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32056049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
